{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# iPhone Tweet Sentiment Classifier\n",
        "\n",
        "This notebook demonstrates a Naive Bayes classifier implementation to analyze sentiment and relevance in tweets about iPhones.\n",
        "\n",
        "Authors: [Enricco Gemha](https://github.com/G3mha), [Marcelo Barranco](https://github.com/Maraba23), [Rafael Leventhal](https://github.com/rafaelcl292)\n",
        "\n",
        "Date: 2021-09-27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Loading Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK packages\n",
        "nltk.download('rslp');\n",
        "nltk.download('stopwords');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Working directory:')\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the dataset containing tweets classified according to the criteria described in the README.md:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "filename = 'iphone.xlsx'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = pd.read_excel(filename)\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.RELEVÂNCIA.value_counts(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "test = pd.read_excel(filename, sheet_name='Teste')\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Automated Sentiment Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classification Categories\n",
        "\n",
        "Product: iPhone\n",
        "\n",
        "- **VERY IRRELEVANT (0)**: Off-topic tweets, unrelated to iPhone, or tweets with minimal content (e.g., just a hashtag)\n",
        "- **IRRELEVANT (1)**: Sales advertisements (e.g., \"Buy now at Magalu\")\n",
        "- **NEUTRAL (2)**: Jokes about iPhone (e.g., \"iPhone is like a mini Corsa lol\")\n",
        "- **RELEVANT (3)**: Indirect comments related to iPhone (e.g., \"My science teacher spent 30 minutes just talking about his new iPhone\")\n",
        "- **VERY RELEVANT (4)**: Direct comments about iPhone - opinions, questions, or purchase intent (e.g., \"iPhone 13 will have to wait a bit longer to reach my hands\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Building a Naive-Bayes Classifier\n",
        "\n",
        "Training the classifier using only the messages from the Training spreadsheet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the Tweet Cleaning Function\n",
        "\n",
        "The function below is responsible for removing:\n",
        "- Links (HTTP, HTTPS, and FTP)\n",
        "- Username tags (@)\n",
        "- Punctuation marks (! - _ . : ? ; [] \\ /)\n",
        "\n",
        "Additionally, the function performs the following processes:\n",
        "- STEMMING (described in NLTK documentation as: \"A processing interface for removing morphological affixes from words.\")\n",
        "- TOKENIZING (described by NLTK documentation as: \"A tokenizer that divides a string into substrings by splitting on the specified string.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_data(text):\n",
        "    # Remove URLs\n",
        "    http_re = r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?'\n",
        "    text = re.sub(http_re, '', text)\n",
        "\n",
        "    # Remove usernames\n",
        "    text = re.sub(r'@[^\\s]*', '', text)\n",
        "\n",
        "    # Remove punctuation marks\n",
        "    text = re.sub(r'[!-_.:?;[\\]/]', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokenizer = nltk.tokenize.casual.TweetTokenizer()\n",
        "    text = tokenizer.tokenize(text)\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = nltk.stem.RSLPStemmer()\n",
        "    text = list(map(stemmer.stem, text))\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preparation\n",
        "train.loc[:, 'Treinamento'] = train.Treinamento.apply(clean_data)\n",
        "train_relevance = train.RELEVÂNCIA.map({\n",
        "    0: 'very irrelevant',\n",
        "    1: 'irrelevant',\n",
        "    2: 'neutral',\n",
        "    3: 'relevant',\n",
        "    4: 'very relevant'\n",
        "})\n",
        "\n",
        "categories = ['very irrelevant', 'irrelevant',\n",
        "              'neutral', 'relevant', 'very relevant']\n",
        "\n",
        "train.loc[:, 'RELEVÂNCIA'] = pd.Categorical(\n",
        "    train_relevance, categories=categories, ordered=True\n",
        ")\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.RELEVÂNCIA.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the Naive Bayes Classifier\n",
        "\n",
        "The process consists of training the classifier based on tweets collected ONLY in the training spreadsheet (train), with the following steps:\n",
        "\n",
        "1. Create a list of relevance categories\n",
        "2. Create a list of words with only one occurrence across all tweets\n",
        "3. Create a dictionary with words for each category\n",
        "4. Create a dictionary with the frequency count of each word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Building the classifier\n",
        "categories = ['very irrelevant', 'irrelevant',\n",
        "              'neutral', 'relevant', 'very relevant']\n",
        "\n",
        "# List of words/emojis present in the entire database\n",
        "total_words = sum(train.Treinamento, [])\n",
        "\n",
        "# Number of unique words/emojis in the entire database\n",
        "unique_words = set(total_words)\n",
        "\n",
        "# Dictionary of words by category\n",
        "words_by_category = {\n",
        "    category: sum(train[train.RELEVÂNCIA == category].Treinamento, [])\n",
        "    for category in categories\n",
        "}\n",
        "\n",
        "# Word occurrence count by category\n",
        "word_occurrence_by_category =  {\n",
        "    category: {\n",
        "        word: words_by_category[category].count(word)\n",
        "        for word in unique_words\n",
        "    }\n",
        "    for category in categories\n",
        "}\n",
        "\n",
        "# Probability by category\n",
        "prob_by_category = {\n",
        "    category: len(words_by_category[category]) / len(total_words)\n",
        "    for category in categories\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Probability Calculation\n",
        "\n",
        "In this section, we define the probability of a phrase belonging to a specific category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function that calculates the probability of a phrase belonging to a given category\n",
        "def prob_phrase(category, phrase):\n",
        "    '''\n",
        "    Calculates the probability of a phrase being in a category\n",
        "    '''\n",
        "    # Clean the phrase if provided as a string\n",
        "    if phrase is str:\n",
        "        phrase = clean_data(phrase)\n",
        "    # Probability calculation\n",
        "    return prob_by_category[category] * np.array(list(\n",
        "        # Probability of each word with Laplace smoothing\n",
        "        (((word_occurrence_by_category[category][word] + 1)\n",
        "        if word in unique_words else 1) /\n",
        "         (len(words_by_category[category]) + len(unique_words)))\n",
        "        for word in phrase\n",
        "    )).prod()  # Product of each word's probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the Classifier Function\n",
        "\n",
        "Finally, we structure the classifier function that returns the category with the highest probability of containing a given phrase, using the probability calculated in the previous cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classifier(phrase):\n",
        "    '''\n",
        "    Returns the category with the highest probability of containing the phrase\n",
        "    '''\n",
        "    return max(\n",
        "        categories, key=lambda category: prob_phrase(category, phrase)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Verifying Classifier Performance\n",
        "\n",
        "Now we test our classifier with the test dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checking the dataframe we'll work with\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparing the test dataframe\n",
        "test.loc[:, 'Teste'] = test.Teste.apply(clean_data)\n",
        "test_relevance = test.RELEVÂNCIA.map(\n",
        "    {0: 'very irrelevant',\n",
        "     1: 'irrelevant',\n",
        "     2: 'neutral',\n",
        "     3: 'relevant',\n",
        "     4: 'very relevant'}\n",
        ")\n",
        "\n",
        "# Creating the test categories\n",
        "test_categories = ['very irrelevant', 'irrelevant',\n",
        "              'neutral', 'relevant', 'very relevant']\n",
        "\n",
        "# Unifying information provided by manual classification\n",
        "test.loc[:, 'RELEVÂNCIA'] = pd.Categorical(\n",
        "    test_relevance, categories=test_categories, ordered=True\n",
        ")\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Applying the classifier to the test table\n",
        "test.loc[:, 'Classifier'] = pd.Categorical(\n",
        "    test.Teste.apply(classifier), categories=categories, ordered=True\n",
        ")\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance of classification on the training set itself\n",
        "sum(train.RELEVÂNCIA == train.Treinamento) / train.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Showing the classifier performance\n",
        "sum(test.RELEVÂNCIA == test.Classifier) / test.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating a crosstab containing the accuracy rate of the test by relevance\n",
        "test['Correct'] = test.Classifier == test.RELEVÂNCIA\n",
        "pd.crosstab(test.Correct, test.RELEVÂNCIA, normalize='columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating a crosstab to evaluate the correspondence of relevance\n",
        "pd.crosstab(test.Classifier, test.RELEVÂNCIA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding the Naive-Bayes Classifier Concept\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The classifier cannot be used to generate more tweets for classification as it would be biased. It wouldn't generate new samples with new words, but rather reuse the same pool of words from the original dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Classifier Quality through New Splits between Training and Test Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# New Iterations\n",
        "We decided to implement randomization in the split between Training and Test tweets, with the accuracy of 100 repetitions. **Results follow below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combining both dataframes, Training and Test\n",
        "df_complete = pd.DataFrame({\n",
        "    'Tweets': train.Treinamento.append(test.Teste, ignore_index=True),\n",
        "    'Relevance': train.RELEVÂNCIA.append(test.RELEVÂNCIA, ignore_index= True),\n",
        "})\n",
        "df_complete.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreating the classification categories\n",
        "# Simplifying to 3 categories by merging very irrelevant with irrelevant and very relevant with relevant\n",
        "\n",
        "new_relevance = []\n",
        "for r in df_complete.Relevance:\n",
        "    if r == \"very irrelevant\":\n",
        "        r = \"irrelevant\"\n",
        "    if r == \"very relevant\":\n",
        "        r = \"relevant\"\n",
        "    new_relevance.append(r)\n",
        "df_complete[\"Relevance\"] = new_relevance\n",
        "df_complete.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Building the list of categories\n",
        "categories = ['irrelevant','neutral', 'relevant']\n",
        "\n",
        "# List for classifier performance\n",
        "performances = []\n",
        "\n",
        "# Running performance test 100 times\n",
        "for _ in range(100):\n",
        "    # Randomizing tweets\n",
        "    df_complete = df_complete.sample(frac=1).reset_index(drop=True)\n",
        "    train = df_complete.iloc[:750]\n",
        "    test = df_complete.iloc[750:1000]\n",
        "\n",
        "    # Total words/emojis in the entire database\n",
        "    total_words = sum(train.Tweets, [])\n",
        "    # Unique words/emojis in the entire database\n",
        "    unique_words = set(total_words)\n",
        "\n",
        "    # Words by category\n",
        "    words_by_category = {\n",
        "        category: sum(train[train.Relevance == category].Tweets, [])\n",
        "        for category in categories\n",
        "    }\n",
        "\n",
        "    # Word occurrence count by category\n",
        "    word_occurrence_by_category =  {\n",
        "        category: {\n",
        "            word: words_by_category[category].count(word)\n",
        "            for word in unique_words\n",
        "        }\n",
        "        for category in categories\n",
        "    }\n",
        "\n",
        "    # Probability by category\n",
        "    prob_by_category = {\n",
        "        category: len(words_by_category[category]) / len(total_words)\n",
        "        for category in categories\n",
        "    }\n",
        "\n",
        "\n",
        "    def prob_phrase(category, phrase):\n",
        "        '''\n",
        "        Calculates the probability of a phrase being in a category\n",
        "        '''\n",
        "        # Clean the phrase if provided as a string\n",
        "        if phrase is str:\n",
        "            phrase = clean_data(phrase)\n",
        "        \n",
        "        # Probability calculation\n",
        "        return prob_by_category[category] * np.array(list(\n",
        "            # Probability of each word with Laplace smoothing\n",
        "            (((word_occurrence_by_category[category][word] + 1)\n",
        "            if word in unique_words else 1) /\n",
        "            (len(words_by_category[category]) + len(unique_words)))\n",
        "            for word in phrase\n",
        "        )).prod()  # Product of each word's probability\n",
        "\n",
        "\n",
        "    def classifier(phrase):\n",
        "        '''\n",
        "        Returns the category with the highest probability of containing the phrase\n",
        "        '''\n",
        "        return max(\n",
        "            categories, key=lambda category: prob_phrase(category, phrase)\n",
        "        )\n",
        "\n",
        "\n",
        "    correct = sum(test.Tweets.apply(classifier) == test.Relevance)\n",
        "    performance = correct / test.shape[0]\n",
        "    performances.append(performance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting histogram for the 100 tests\n",
        "plt.hist(performances, edgecolor='white', density=True)\n",
        "plt.title('Histogram of performance obtained from 100 different classifier iterations')\n",
        "plt.ylabel('Density')\n",
        "plt.xlabel('Performance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting some important statistics from the 100 tests to support the histogram above\n",
        "pd.Series(performances).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p>\n",
        "<hr></hr>\n",
        "</p>\n",
        "\n",
        "# CONCLUSION\n",
        "<p>\n",
        "<hr></hr>\n",
        "</p>\n",
        "\n",
        "## Best Approach for Classifier Construction\n",
        "Based on the graph above and the performance metrics obtained throughout this notebook, we can infer that this method of randomization and multiple testing helps IMPROVE the performance of the Naive Bayes classifier by reducing bias and increasing the test area.\n",
        "\n",
        "\n",
        "## Applications of the Classifier Beyond This Project\n",
        "Based on what we've presented in this notebook, we can state that the applications of the Naive Bayes classifier are extensive and relevant. For example, a company could employ this classifier to evaluate the performance of a product or advertising campaign. Furthermore, this classifier could find applications in social media engagement algorithms, as mentioned in Fabio Akita's blog post (see references). It's also worth mentioning the various possible applications in healthcare, such as disease test inspection (calculating false positive/negative probabilities), behavioral science, mental health conditions like Alzheimer's, personality disorder studies, and the well-known spam filtering in email inboxes.\n",
        "\n",
        "## Real Improvements for the Naive Bayes Classifier\n",
        "Throughout the project, we observed that the classifier has some limitations, such as when a tweet is ironic or contains double negation, which can lead to incorrect classification. Additionally, to improve the classifier's accuracy, the quantity of classified tweets should be expanded, both manually and through training, to minimize potential errors and inconsistencies inherent in the tweet separation process. Finally, it's worth mentioning the need to implement the Monte Carlo method, which involves generating random numbers between 0 and 1, and if the number is less than the probability of a given tweet being relevant, the tweet will be classified as relevant, and vice versa. This method becomes considerably more effective as the database grows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# References\n",
        "\n",
        "<a href=\"https://arxiv.org/pdf/1410.5329.pdf\">Naive Bayes and Text Classification</a><br> **More comprehensive**\n",
        "\n",
        "\n",
        "<a href=\"https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\">A practical explanation of a Naive Bayes Classifier</a><br> **Simpler explanation**\n",
        "\n",
        "\n",
        "<a href=\"https://www.akitaonrails.com/2020/09/30/akitando-84-entendendo-o-dilema-social-e-como-voce-e-manipulado\">Blog post on Bayesian theorem applications by Fabio Akita</a><br>\n",
        "\n",
        "\n",
        "<a href=\"https://www.youtube.com/watch?v=HZGCoVF3YvM\">3Blue1Brown: Bayes Theorem</a><br>\n",
        "\n",
        "\n",
        "<a href=\"https://www.youtube.com/watch?v=R13BD8qKeTg\">Veritasium: The Bayesian Trap</a><br>\n"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
